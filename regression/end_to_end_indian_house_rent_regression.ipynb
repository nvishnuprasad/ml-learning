{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7f5f41",
   "metadata": {},
   "source": [
    "# City House Rent Prediction (India) — End-to-End Regression Project\n",
    "\n",
    "We will build a model that predicts **monthly rent (₹)** from rental listing attributes.\n",
    "\n",
    "This notebook is written like a real ML project notebook:\n",
    "- short code cells\n",
    "- clear narration\n",
    "- **inference after key outputs**\n",
    "- train / validation / test split\n",
    "- cross-validation for model comparison\n",
    "- pipelines for preprocessing\n",
    "\n",
    "We keep the math light. We focus on decisions and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db55dd6",
   "metadata": {},
   "source": [
    "## 1) Load the dataset\n",
    "\n",
    "We start by loading the CSV into a DataFrame.\n",
    "Then we preview a few rows to understand the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is the standard library for working with tables in Python.\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = r\"data\\cities_magicbricks_rental_prices.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe21cc",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - `rent` is the target we want to predict. It is continuous → **regression**.\n",
    "> - We have numeric features (`area`, `beds`, …) and categorical features (`city`, `furnishing`, …).\n",
    "> - `house_type` is free text. We will skip it in the first baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f54f0",
   "metadata": {},
   "source": [
    "## 2) Quick data checks\n",
    "\n",
    "We check size, data types, and missing values.\n",
    "This tells us what preprocessing we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick dataset size summary (readable format)\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "rows, cols = df.shape\n",
    "display(Markdown(f\"**Dataset size:** `{rows:,}` rows × `{cols:,}` columns\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5e0b4",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - More rows usually means more stable model estimates.\n",
    "> - A small dataset can make model comparisons noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic schema overview\n",
    "df.info()\n",
    "\n",
    "# Rich schema summary (types + missingness)\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "schema = pd.DataFrame({\n",
    "    \"column\": df.columns,\n",
    "    \"dtype\": [str(t) for t in df.dtypes],\n",
    "    \"missing\": df.isna().sum().values,\n",
    "    \"missing_%\": (df.isna().mean() * 100).round(2).values,\n",
    "})\n",
    "schema = schema.sort_values([\"missing\", \"column\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "display(\n",
    "    schema.style\n",
    "        .format({\"missing\": \"{:,}\", \"missing_%\": \"{:.2f}%\"})\n",
    "        .bar(subset=[\"missing_%\"], align=\"mid\")\n",
    "        .set_caption(\"Schema summary (sorted by missingness)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b324d7",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - Categorical columns typically show up as `object`.\n",
    "> - Numeric columns show up as `int64` or `float64`.\n",
    "> - You should see a **small amount of missing values**.\n",
    "> We injected them on purpose so we can demonstrate missing-value handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1fee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values by column (top)\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "missing = (df.isna().sum().sort_values(ascending=False)).reset_index()\n",
    "missing.columns = [\"column\", \"missing_count\"]\n",
    "missing[\"missing_%\"] = (missing[\"missing_count\"] / len(df) * 100).round(2)\n",
    "\n",
    "display(\n",
    "    missing.head(12).style\n",
    "        .format({\"missing_count\": \"{:,}\", \"missing_%\": \"{:.2f}%\"})\n",
    "        .bar(subset=[\"missing_%\"], align=\"mid\")\n",
    "        .set_caption(\"Top columns with missing values\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9fab4",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - Missing values are scattered.\n",
    "> - This is common in real data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6dab8c",
   "metadata": {},
   "source": [
    "## 3) Handling missing data (explicit section)\n",
    "\n",
    "Missing data is normal in real projects.\n",
    "\n",
    "Common strategies:\n",
    "1. **Drop** rows or columns (fast, but you may lose signal)  \n",
    "2. **Impute** missing values (simple and effective for baselines)  \n",
    "3. **Model-based imputation** (KNN, MICE, etc. — more complex)\n",
    "\n",
    "In this notebook:\n",
    "- For **numeric** columns, we will impute using the **mean**.\n",
    "- For **categorical** columns, we will impute using the **most frequent** category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b46c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missingness percentage (top)\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "missing_pct = (df.isna().mean() * 100).sort_values(ascending=False).reset_index()\n",
    "missing_pct.columns = [\"column\", \"missing_%\"]\n",
    "\n",
    "display(\n",
    "    missing_pct.head(12).style\n",
    "        .format({\"missing_%\": \"{:.2f}%\"})\n",
    "        .bar(subset=[\"missing_%\"], align=\"mid\")\n",
    "        .set_caption(\"Missingness percentage (top columns)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1515f0ef",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - If missingness is very high in a column (say 40%+), consider dropping it or collecting better data.\n",
    "> - Here, missingness is intentionally small. This makes imputation a reasonable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16073c9e",
   "metadata": {},
   "source": [
    "### 3.1 Simple imputation demo (before pipelines)\n",
    "\n",
    "We will demonstrate imputation on a small subset.\n",
    "Later, we will do this properly inside a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleImputer is scikit-learn's standard tool for filling missing values.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_cols = [\"area\", \"beds\", \"bathrooms\", \"balconies\", \"area_rate\"]\n",
    "numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
    "\n",
    "cat_cols = [\"city\", \"furnishing\"]\n",
    "cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "df_num_demo = pd.DataFrame(num_imputer.fit_transform(df[numeric_cols]), columns=numeric_cols)\n",
    "df_cat_demo = pd.DataFrame(cat_imputer.fit_transform(df[cat_cols]), columns=cat_cols)\n",
    "\n",
    "(df[numeric_cols].isna().sum().sum(), df_num_demo.isna().sum().sum()), (df[cat_cols].isna().sum().sum(), df_cat_demo.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3e2cd9",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - After imputation, the demo subsets contain **no missing values**.\n",
    "> - This is exactly what we want before training most models.\n",
    "> - We will do the same thing again inside a pipeline (the safer approach)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ff076",
   "metadata": {},
   "source": [
    "## 4) Summary statistics\n",
    "\n",
    "We inspect numeric summaries to spot outliers and scale differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e8657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics (styled)\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "summary = df.describe(include=\"all\").transpose()\n",
    "# Show a compact view first (top rows) then let user scroll in output if needed\n",
    "display(\n",
    "    summary.style\n",
    "        .set_caption(\"Summary statistics (all columns)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b740b279",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - Rent often has a long right tail (premium listings).\n",
    "> - Outliers can dominate RMSE.\n",
    "> - Category counts can be unbalanced (some cities have far more listings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbd8aa",
   "metadata": {},
   "source": [
    "## 5) Split into train / validation / test\n",
    "\n",
    "We use three splits:\n",
    "- **train**: fit the model  \n",
    "- **validation**: compare models / choices  \n",
    "- **test**: final unbiased evaluation  \n",
    "\n",
    "We keep the test set untouched until the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split gives a reproducible split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=[\"rent\"])\n",
    "y = df[\"rent\"]\n",
    "\n",
    "# 20% test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# From remaining 80%, take 25% as validation -> 20% overall\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "(len(X_train), len(X_val), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedab43",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - This is a safe pattern: we do not tune on the test set.\n",
    "> - Validation is where we compare models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9462f066",
   "metadata": {},
   "source": [
    "## 6) Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA builds intuition.\n",
    "We ask:\n",
    "- What does rent look like?\n",
    "- Which numeric features move rent?\n",
    "- How do cities and furnishing levels differ?\n",
    "\n",
    "We will use **Seaborn** for cleaner, more “report-like” visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50350df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn sits on top of Matplotlib and gives nicer default plots.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "def format_inr(x, pos=None):\n",
    "    try:\n",
    "        return \"₹ {:,}\".format(int(x))\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "inr_formatter = FuncFormatter(format_inr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e892e5",
   "metadata": {},
   "source": [
    "### 6.1 Distribution of rent (histogram + box)\n",
    "\n",
    "This is inspired by many Kaggle EDA notebooks:\n",
    "- histogram for shape\n",
    "- boxplot for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe45a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(11, 6))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[4, 1], hspace=0.05)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax2 = fig.add_subplot(gs[1], sharex=ax1)\n",
    "\n",
    "sns.histplot(y_train, bins=50, kde=True, ax=ax1, color=\"#4C78A8\")\n",
    "ax1.set_title(\"Distribution of Monthly Rent (Train Set)\")\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.xaxis.set_major_formatter(inr_formatter)\n",
    "\n",
    "sns.boxplot(x=y_train, ax=ax2, color=\"#72B7B2\")\n",
    "ax2.set_xlabel(\"Monthly Rent (₹)\")\n",
    "ax2.xaxis.set_major_formatter(inr_formatter)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417dd9c5",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - The distribution is right-skewed. A few premium rentals exist.\n",
    "> - The boxplot makes outliers obvious.\n",
    "> - RMSE will be sensitive to extreme rents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a15c88",
   "metadata": {},
   "source": [
    "### 6.2 Numeric features vs rent\n",
    "\n",
    "We plot rent against each numeric feature.\n",
    "We also add a trend line to make the direction clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3afcca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\"area\", \"beds\", \"bathrooms\", \"balconies\", \"area_rate\"]\n",
    "numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(10, 5.5))\n",
    "    sns.regplot(\n",
    "        data=df,\n",
    "        x=col,\n",
    "        y=\"rent\",\n",
    "        scatter_kws={\"alpha\": 0.25, \"s\": 18},\n",
    "        line_kws={\"linewidth\": 2},\n",
    "        color=\"#F58518\"\n",
    "    )\n",
    "    plt.gca().yaxis.set_major_formatter(inr_formatter)\n",
    "    plt.title(f\"{col} vs Rent (with trend)\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Monthly Rent (₹)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44372daf",
   "metadata": {},
   "source": [
    "### 6.2.1 A 3D view (rent vs area vs area_rate)\n",
    "\n",
    "A 3D scatter can be a fun way to see how **two** inputs relate to rent at the same time.\n",
    "Use it for intuition, not for precise conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd44e9-c5f6-415d-a645-c5c762eb955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.io as pio\n",
    "\n",
    "print(\"Plotly version:\", plotly.__version__)\n",
    "print(\"Available renderers:\", pio.renderers)\n",
    "\n",
    "# Best defaults for JupyterLab:\n",
    "# Try one of these (first one usually works)\n",
    "pio.renderers.default = \"notebook_connected\"   # good for JupyterLab + classic\n",
    "#pio.renderers.default = \"jupyterlab\"         # works in some setups\n",
    "#pio.renderers.default = \"iframe\"             # always works, slightly heavier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d595d-81f1-4b48-8d52-c3bf901c73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Prepare clean data for 3D visualization\n",
    "cols3d = [\"area\", \"area_rate\", \"rent\"]\n",
    "df3d = df[cols3d + [\"city\"]].dropna().copy()\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df3d,\n",
    "    x=\"area\",\n",
    "    y=\"area_rate\",\n",
    "    z=\"rent\",\n",
    "    color=\"city\",                     # remove this line if you want single-color points\n",
    "    opacity=0.5,\n",
    "    title=\"3D View: Area vs Area Rate vs Rent\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Area (sqft)\",\n",
    "        yaxis_title=\"Area Rate\",\n",
    "        zaxis_title=\"Monthly Rent (₹)\",\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=40),\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b3a896",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - Listings with similar area can have very different rent if the area rate differs.\n",
    "> - This supports why both `area` and `area_rate` can be useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a5976",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - `area` typically shows the clearest upward trend.\n",
    "> - Discrete counts (`beds`, `bathrooms`) create “bands”.\n",
    "> - The trend line is a guide. It does not guarantee linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b69fa",
   "metadata": {},
   "source": [
    "### 6.3 Categorical overview (counts)\n",
    "\n",
    "We check how many categories exist.\n",
    "Then we visualize the top counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f62f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Explicitly include both legacy object and new string dtypes\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
    "\n",
    "cardinality = (\n",
    "    pd.DataFrame({\n",
    "        \"column\": categorical_cols,\n",
    "        \"unique_categories\": [df[c].nunique(dropna=True) for c in categorical_cols]\n",
    "    })\n",
    "    .sort_values(\"unique_categories\", ascending=False)\n",
    ")\n",
    "\n",
    "display(\n",
    "    cardinality.style\n",
    "        .format({\"unique_categories\": \"{:,}\"})\n",
    "        .bar(subset=[\"unique_categories\"], align=\"mid\")\n",
    "        .set_caption(\"Category counts (unique values)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f8fdd",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - `city` and `furnishing` are low-cardinality. They are great for one-hot encoding.\n",
    "> - `locality` can have many categories. We will skip it in the first baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cities = (\n",
    "    df[\"city\"]\n",
    "    .value_counts()\n",
    "    .head(12)\n",
    "    .reset_index()\n",
    ")\n",
    "top_cities.columns = [\"city\", \"count\"]\n",
    "\n",
    "plt.figure(figsize=(11, 5.5))\n",
    "sns.barplot(\n",
    "    data=top_cities,\n",
    "    x=\"city\",\n",
    "    y=\"count\",\n",
    "    hue=\"city\",\n",
    "    palette=\"viridis\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Top Cities by Listing Count\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.ylabel(\"Number of Listings\")\n",
    "plt.xticks(rotation=25, ha=\"right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea66efdf",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - If one city dominates, the model may become city-biased.\n",
    "> - Later, check error by city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d868d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8.5, 5))\n",
    "sns.countplot(\n",
    "    data=df,\n",
    "    x=\"furnishing\",\n",
    "    hue=\"furnishing\",     # explicit hue\n",
    "    order=df[\"furnishing\"].value_counts().index,\n",
    "    palette=\"Set2\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Furnishing Distribution\")\n",
    "plt.xlabel(\"Furnishing\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=15, ha=\"right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b949a31",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - Furnishing often shifts the rent baseline.\n",
    "> - This is a strong reason to keep it as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9919d7",
   "metadata": {},
   "source": [
    "### 6.4 Rent by category (boxplots)\n",
    "\n",
    "Boxplots show how rent spreads across categories.\n",
    "We plot the top cities for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4edfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_city_names = df[\"city\"].value_counts().head(8).index.tolist()\n",
    "df_top = df[df[\"city\"].isin(top_city_names)].copy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_top, x=\"city\", y=\"rent\", hue=\"city\", palette=\"pastel\", showfliers=False, legend=False)\n",
    "plt.gca().yaxis.set_major_formatter(inr_formatter)\n",
    "plt.title(\"Rent Distribution by City (Top 8 Cities)\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.ylabel(\"Monthly Rent (₹)\")\n",
    "plt.xticks(rotation=25, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7926b86",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - Some cities have a clearly higher rent range.\n",
    "> - A global model needs to learn both city-level shifts and within-city trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19624f58",
   "metadata": {},
   "source": [
    "## Key insights from EDA\n",
    "\n",
    "Before we model, we write down a few observations from the charts.\n",
    "This keeps the workflow realistic: **EDA should influence decisions**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d225b",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - Note the top cities. A model may learn dominant cities better than rare ones.\n",
    "> - Compare median rent across furnishing levels. If the gaps are large, furnishing should help prediction.\n",
    "> - Check which numeric feature correlates most with rent (often `area` or `area_rate`).\n",
    "> - Confirm missingness exists. This justifies imputation steps later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a881f15",
   "metadata": {},
   "source": [
    "## 7) Correlation (numeric features only)\n",
    "\n",
    "Correlation is a quick way to rank numeric relationships.\n",
    "It does not prove causation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57404be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with rent (numeric only)\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "corr = df[numeric_cols + [\"rent\"]].corr(numeric_only=True)\n",
    "\n",
    "corr_with_rent = corr[\"rent\"].drop(\"rent\").sort_values(ascending=False).reset_index()\n",
    "corr_with_rent.columns = [\"feature\", \"correlation_with_rent\"]\n",
    "\n",
    "display(\n",
    "    corr_with_rent.style\n",
    "        .format({\"correlation_with_rent\": \"{:.3f}\"})\n",
    "        .bar(subset=[\"correlation_with_rent\"], align=\"mid\")\n",
    "        .set_caption(\"Numeric correlation with rent (higher magnitude → stronger linear relationship)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62e843",
   "metadata": {},
   "source": [
    "### 7.1 Correlation heatmap (numeric features)\n",
    "\n",
    "A heatmap makes correlation patterns easier to scan.\n",
    "We only include numeric columns here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A heatmap is a compact way to view correlations.\n",
    "# We keep it to numeric columns to avoid mixing types.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, cbar=True)\n",
    "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6085c70",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - `area` and `area_rate` often rank high.\n",
    "> - Categorical effects do not show here. Use boxplots for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769d2de",
   "metadata": {},
   "source": [
    "## 8) Feature selection (baseline model)\n",
    "\n",
    "We start with a small, stable set of features.\n",
    "\n",
    "We use:\n",
    "- numeric: `area`, `beds`, `bathrooms`, `balconies`, `area_rate`\n",
    "- categorical: `city`, `furnishing`\n",
    "\n",
    "We skip for now:\n",
    "- `locality` (high-cardinality)\n",
    "- `house_type` (free text)\n",
    "\n",
    "Future idea:\n",
    "- Use `locality` to map to a **pincode** or zone, then use grouped features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8751f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [\"area\", \"beds\", \"bathrooms\", \"balconies\", \"area_rate\"]\n",
    "cat_features = [\"city\", \"furnishing\"]\n",
    "\n",
    "X_train_sel = X_train[num_features + cat_features].copy()\n",
    "X_val_sel   = X_val[num_features + cat_features].copy()\n",
    "X_test_sel  = X_test[num_features + cat_features].copy()\n",
    "\n",
    "X_train_sel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1b03d",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - Baselines should be simple and explainable.\n",
    "> - You can add complexity later once you trust the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26644f34",
   "metadata": {},
   "source": [
    "## 9) Preprocessing with pipelines (imputation + one-hot)\n",
    "\n",
    "A pipeline chains steps safely.\n",
    "It also avoids data leakage.\n",
    "\n",
    "Our preprocessor will:\n",
    "- impute numeric missing values with the **mean**\n",
    "- impute categorical missing values with the **most frequent**\n",
    "- one-hot encode categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_pipeline, num_features),\n",
    "    (\"cat\", categorical_pipeline, cat_features),\n",
    "])\n",
    "\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4102445",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - We do not clean the DataFrame by hand.\n",
    "> - The pipeline learns imputation values from training data only.\n",
    "> - `handle_unknown=\"ignore\"` makes inference safer if a new city appears later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fce9448",
   "metadata": {},
   "source": [
    "## 10) Cross-validation (CV) for model comparison\n",
    "\n",
    "Cross-validation trains the model multiple times on different folds of the training set.\n",
    "This reduces the risk of a lucky or unlucky split.\n",
    "\n",
    "We compare three models:\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Decision Tree Regressor\n",
    "\n",
    "Metric: RMSE (lower is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb16d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad60839",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "candidates = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge(alpha=1.0)\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "def cv_rmse_scores(model, X, y):\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", model)])\n",
    "    scores = cross_val_score(pipe, X, y, cv=cv, scoring=\"neg_mean_absolute_error\")\n",
    "    return -scores  # convert to positive MAE\n",
    "\n",
    "cv_scores = {name: cv_rmse_scores(model, X_train_sel, y_train) for name, model in candidates.items()}\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba6d31",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - You see fold-by-fold RMSE for each model.\n",
    "> - Lower RMSE is better.\n",
    "> - High spread across folds means instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 6))\n",
    "sns.boxplot(data=pd.DataFrame(cv_scores), palette=\"Set3\")\n",
    "plt.title(\"Cross-Validation MAE by Model (Train Set)\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.show()\n",
    "\n",
    "[(k, float(np.mean(v)), float(np.std(v))) for k, v in cv_scores.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f692391",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - Prefer models with low mean RMSE and low variance.\n",
    "> - Trees can overfit and show higher variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d9bc4",
   "metadata": {},
   "source": [
    "## 11) Train each model and evaluate on the validation set\n",
    "\n",
    "Now we train on the training split once.\n",
    "Then we score on the validation split.\n",
    "This is a realistic model-selection loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd241314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "def train_and_eval(model, X_tr, y_tr, X_eval, y_eval):\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", model)])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    preds = pipe.predict(X_eval)\n",
    "    mae = float(mean_absolute_error(y_eval, preds))\n",
    "    r2 = float(r2_score(y_eval, preds))\n",
    "    return pipe, preds, mae, r2\n",
    "\n",
    "val_results = []\n",
    "val_preds = {}\n",
    "trained = {}\n",
    "\n",
    "for name, model in candidates.items():\n",
    "    pipe, preds, mae, r2 = train_and_eval(model, X_train_sel, y_train, X_val_sel, y_val)\n",
    "    trained[name] = pipe\n",
    "    val_preds[name] = preds\n",
    "    val_results.append((name, mae, r2))\n",
    "\n",
    "sorted(val_results, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdeb2db",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - Validation RMSE is easiest to interpret in ₹.\n",
    "> - R² is a sanity check. Do not optimize for it blindly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9688cfa",
   "metadata": {},
   "source": [
    "### 11.1 Validation residuals (per model)\n",
    "\n",
    "Residual = actual − predicted.\n",
    "We want residuals centered around 0 with reasonable spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for name, preds in val_preds.items():\n",
    "    residuals = y_val.values - preds\n",
    "    sns.histplot(residuals, bins=35, stat=\"count\", element=\"step\", fill=False, label=name)\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(inr_formatter)\n",
    "plt.title(\"Validation Residual Distributions (Actual − Predicted)\")\n",
    "plt.xlabel(\"Residual (₹)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1e966",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - Narrower residual distributions usually mean lower error.\n",
    "> - Long tails usually come from premium listings and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b7524a",
   "metadata": {},
   "source": [
    "## 12) Pick the best model and evaluate on the test set\n",
    "\n",
    "We pick the model with the best validation RMSE.\n",
    "Then we refit it on train + validation.\n",
    "Finally, we evaluate once on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e80f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name, best_rmse, best_r2 = sorted(val_results, key=lambda x: x[1])[0]\n",
    "best_name, best_rmse, best_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d413a9",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - After this point, we stop comparing models.\n",
    "> - The test set is used only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62bd82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = candidates[best_name]\n",
    "\n",
    "X_trainval_sel = X_trainval[num_features + cat_features].copy()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "final_pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", best_model)])\n",
    "final_pipe.fit(X_trainval_sel, y_trainval)\n",
    "\n",
    "test_preds = final_pipe.predict(X_test_sel)\n",
    "\n",
    "test_rmse = float(mean_absolute_error(y_test, test_preds))\n",
    "test_r2 = float(r2_score(y_test, test_preds))\n",
    "\n",
    "test_rmse, test_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0fee62",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - This test RMSE is your final unbiased estimate.\n",
    "> - Do not tune hyperparameters using the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ad95c",
   "metadata": {},
   "source": [
    "## 13) Error analysis on the test set\n",
    "\n",
    "We inspect:\n",
    "- residual distribution\n",
    "- biggest mistakes (largest absolute error)\n",
    "- average error by city (quick check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d648b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_residuals = y_test.values - test_preds\n",
    "\n",
    "plt.figure(figsize=(11, 5.5))\n",
    "sns.histplot(test_residuals, bins=40, kde=True, color=\"#B279A2\")\n",
    "plt.gca().xaxis.set_major_formatter(inr_formatter)\n",
    "plt.title(f\"Test Residual Distribution — {best_name}\")\n",
    "plt.xlabel(\"Residual (₹)  (Actual − Predicted)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b6a32c",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - If residuals are shifted left or right, the model is biased.\n",
    "> - Wide spread means the model is not precise.\n",
    "> - Long tails indicate outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65387ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biggest errors (top 15)\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reconstruct the error table explicitly (safe if cells are run out of order)\n",
    "errors = X_test_sel.copy()\n",
    "errors[\"actual_rent\"] = y_test.values\n",
    "errors[\"predicted_rent\"] = test_preds\n",
    "errors[\"abs_error\"] = np.abs(errors[\"actual_rent\"] - errors[\"predicted_rent\"])\n",
    "\n",
    "top_err = errors.sort_values(\"abs_error\", ascending=False).head(15).copy()\n",
    "\n",
    "display(\n",
    "    top_err.style\n",
    "        .format({\n",
    "            \"actual_rent\": \"₹ {:,.0f}\",\n",
    "            \"predicted_rent\": \"₹ {:,.0f}\",\n",
    "            \"abs_error\": \"₹ {:,.0f}\",\n",
    "        })\n",
    "        .bar(subset=[\"abs_error\"], align=\"mid\")\n",
    "        .set_caption(\"Largest absolute errors on the test set\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc0576",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - This table is actionable.\n",
    "> - It shows which listings need better features or a stronger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14184c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"city\" in errors.columns:\n",
    "    city_mae = errors.groupby(\"city\")[\"abs_error\"].mean().sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10.5, 5.5))\n",
    "    sns.barplot(x=city_mae.index[:10], y=city_mae.values[:10], hue=city_mae.index[:10], palette=\"magma\", legend=False)\n",
    "    plt.gca().yaxis.set_major_formatter(inr_formatter)\n",
    "    plt.title(\"Average Absolute Error by City (Top 10) — Test Set\")\n",
    "    plt.xlabel(\"City\")\n",
    "    plt.ylabel(\"Avg Absolute Error (₹)\")\n",
    "    plt.xticks(rotation=25, ha=\"right\")\n",
    "    plt.show()\n",
    "\n",
    "    city_mae.head(10)\n",
    "\n",
    "# Also show the table for quick reading\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "city_mae_table = city_mae.head(10).reset_index()\n",
    "city_mae_table.columns = [\"city\", \"avg_abs_error\"]\n",
    "\n",
    "display(\n",
    "    city_mae_table.style\n",
    "        .format({\"avg_abs_error\": \"₹ {:,.0f}\"})\n",
    "        .bar(subset=[\"avg_abs_error\"], align=\"mid\")\n",
    "        .set_caption(\"Avg absolute error by city (top 10) — test set\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7209345",
   "metadata": {},
   "source": [
    "> **Inference**\n",
    "> - If one city has much higher error, you may need more data or better features for that city.\n",
    "> - This is a common production issue for geo-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c7a26",
   "metadata": {},
   "source": [
    "## 14) Recap and next steps\n",
    "\n",
    "You built a realistic ML workflow:\n",
    "- EDA with readable plots\n",
    "- explicit missing data handling section\n",
    "- train / validation / test split\n",
    "- preprocessing pipeline (impute + one-hot)\n",
    "- model comparison using cross-validation\n",
    "- final evaluation on the test set\n",
    "- error analysis\n",
    "\n",
    "**Next steps**\n",
    "- Try stronger models (RandomForest, GradientBoosting)\n",
    "- Engineer features (log(rent), rent per sqft)\n",
    "- Add locality carefully (high-cardinality strategies)\n",
    "- Map locality to zones/pincodes for richer location signals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
